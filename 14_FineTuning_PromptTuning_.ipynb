{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/LLM-Agents/blob/main/14_FineTuning_PromptTuning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCA0sm55VaS-",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "\n",
        "Models: bloomz-560m / bloomz-1b\n",
        "\n",
        "Colab Environment: GPU L4.\n",
        "\n",
        "Keys:\n",
        "* PEFT\n",
        "* Prompt tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
          "showTitle": false,
          "title": ""
        },
        "id": "2vkOvTEsVaTA"
      },
      "source": [
        "# Prompt Tuning\n",
        "\n",
        "## Brief introduction to Prompt Tuning.\n",
        "It’s an Additive Fine-Tuning technique for models. This means that we WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL. You might be wondering, how are we going to perform fine-tuning then? Well, we will train additional layers that are added to the model. That’s why it’s called an Additive technique.\n",
        "\n",
        "Considering it’s an Additive technique and its name is Prompt-Tuning, it seems clear that the layers we’re going to add and train are related to the prompt.\n",
        "\n",
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_5_Prompt_Tuning.jpg?raw=true)\n",
        "\n",
        "We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, that particular section of the prompt cannot be translated into natural language. **It's as if we've mastered expressing ourselves in embeddings and generating highly effective prompts.**\n",
        "\n",
        "In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt.\n",
        "\n",
        "The primary consequence of this technique is that the number of parameters to train is genuinely small. However, we encounter a second, perhaps more significant consequence, namely that, **since we do not modify the weights of the pretrained model, it does not alter its behavior or forget any information it has previously learned.**\n",
        "\n",
        "The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered\n",
        "\n",
        "## What are we going to do in the notebook?\n",
        "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One will be trained to generate prompts and the other to detect hate in sentences.\n",
        "\n",
        "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZhdbTh-VaTA"
      },
      "source": [
        "## Loading the Peft Library\n",
        "This library contains the Hugging Face implementation of various fine-tuning techniques, including Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "JechhJhhVaTA"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.10.0\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q accelerate==0.29.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGbh426RVaTB"
      },
      "source": [
        "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "id": "KWOEt-yOVaTB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYsnwjSVaTC"
      },
      "source": [
        "## Loading the model and the tokenizers.\n",
        "\n",
        "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning.\n",
        "\n",
        "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab. Feel Free to try with a bigger one if you have acces to a good GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnqIhv2UVaTC"
      },
      "outputs": [],
      "source": [
        "model_name = \"bigscience/bloomz-560m\"\n",
        "#model_name = \"bigscience/bloomz-1b1\"\n",
        "NUM_VIRTUAL_TOKENS = 20\n",
        "#If you just want to test the solution, you can reduce the EPOCHs.\n",
        "NUM_EPOCHS_PROMPT = 10\n",
        "NUM_EPOCHS_CLASSIFIER = 10\n",
        "device = \"cuda\" #Replace with \"mps\" for Silicon chips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSMu3qRsVaTC"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map = device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2fWhOnVaTC"
      },
      "source": [
        "## Inference with the pre trained bloom model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47j2D3WWVaTC"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
          "showTitle": false,
          "title": ""
        },
        "id": "kRLSfuo2VaTC"
      },
      "source": [
        "To compare the pre-trained model with the same model after the prompt-tuning process, I will run the same sentence on both models.\n",
        "\n",
        "Since I'm creating a model that can generate prompts, I'll instruct it to provide a prompt that makes it act like a fitness trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
          "showTitle": false,
          "title": ""
        },
        "id": "QvStaT7cVaTC"
      },
      "outputs": [],
      "source": [
        "input_prompt = tokenizer(\"Act as a fitness Trainer. Prompt: \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_prompt.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-PY1yAh2mB"
      },
      "source": [
        "The model doesn't know what its mission is and answers as best as it can. It's not a bad response, but it's not what we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BukcAFSja3a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
          "showTitle": false,
          "title": ""
        },
        "id": "OGbJTbRnVaTD"
      },
      "source": [
        "# Prompt Creator\n",
        "## Preparing Datasets\n",
        "The Dataset used, for this first example, is:\n",
        "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD8H_LLaVaTD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
          "showTitle": false,
          "title": ""
        },
        "id": "xmAp_o4PVaTD"
      },
      "outputs": [],
      "source": [
        "dataset_prompt = \"fka/awesome-chatgpt-prompts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaU4FmgwAzZK"
      },
      "outputs": [],
      "source": [
        "def concatenate_columns_prompt(dataset):\n",
        "    def concatenate(example):\n",
        "        example['prompt'] = \"Act as a {}. Prompt: {}\".format(example['act'], example['prompt'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoL6qitsLo0o"
      },
      "outputs": [],
      "source": [
        "#Create the Dataset to create prompts.\n",
        "data_prompt = load_dataset(dataset_prompt)\n",
        "data_prompt['train'] = concatenate_columns_prompt(data_prompt['train'])\n",
        "\n",
        "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]),\n",
        "                              batched=True)\n",
        "train_sample_prompt = data_prompt[\"train\"].remove_columns('act')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL1CwC9tGGSn"
      },
      "outputs": [],
      "source": [
        "print(train_sample_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzA3rLTk8XW8"
      },
      "outputs": [],
      "source": [
        "print(train_sample_prompt[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
          "showTitle": false,
          "title": ""
        },
        "id": "0-5mv1ZpVaTD"
      },
      "source": [
        "## prompt-tuning configuration.  \n",
        "\n",
        "API docs:\n",
        "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
          "showTitle": false,
          "title": ""
        },
        "id": "sOg1Yh-oVaTD"
      },
      "outputs": [],
      "source": [
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "generation_config_prompt = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9KBtB1VaTD"
      },
      "source": [
        "We will create two  prompt tuning models using the same pre-trained model and the same config, but with a different Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_D8oDQZVaTD"
      },
      "outputs": [],
      "source": [
        "peft_model_prompt = get_peft_model(foundational_model, generation_config_prompt)\n",
        "print(peft_model_prompt.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
          "showTitle": false,
          "title": ""
        },
        "id": "i6WhJSUwVaTE"
      },
      "source": [
        "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
        "\n",
        "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJoznfzjVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        #use_cpu=True, # This is necessary for CPU clusters.\n",
        "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
        "        #per_device_train_batch_size=4,\n",
        "        num_train_epochs=epochs\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
          "showTitle": false,
          "title": ""
        },
        "id": "cb1j50DSVaTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "working_dir = \"./\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
        "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)\n",
        "if not os.path.exists(output_directory_prompt):\n",
        "    os.mkdir(output_directory_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5IhO9mVaTE"
      },
      "source": [
        "We need to indicate the directory containing the model when creating the TrainingArguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
          "showTitle": false,
          "title": ""
        },
        "id": "GdMfjk5RVaTE"
      },
      "source": [
        "## Training first model\n",
        "\n",
        "We will create the trainer Object, one for each model to train.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4v4RSSeVaTE"
      },
      "outputs": [],
      "source": [
        "training_args_prompt = create_training_arguments(output_directory_prompt,\n",
        "                                                 3e-2,\n",
        "                                                 NUM_EPOCHS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVAfNdEIVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to train the model.\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        "    )\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnVIwpVSHZ_V"
      },
      "outputs": [],
      "source": [
        "train_sample_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
          "showTitle": false,
          "title": ""
        },
        "id": "1Sz9BeFZVaTF"
      },
      "outputs": [],
      "source": [
        "#Training first model.\n",
        "trainer_prompt = create_trainer(peft_model_prompt,\n",
        "                                training_args_prompt,\n",
        "                                train_sample_prompt)\n",
        "trainer_prompt.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veg8ziHvWh4I"
      },
      "source": [
        "Release GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYVcJDSP7Sq-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
          "showTitle": false,
          "title": ""
        },
        "id": "s5k10HwoVaTG"
      },
      "source": [
        "## Save model\n",
        "We are going to save the model. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
          "showTitle": false,
          "title": ""
        },
        "id": "E3dn3PeMVaTG"
      },
      "outputs": [],
      "source": [
        "trainer_prompt.model.save_pretrained(output_directory_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
          "showTitle": false,
          "title": ""
        },
        "id": "rkUKpDDWVaTG"
      },
      "source": [
        "## Inference first tuned model\n",
        "\n",
        "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
          "showTitle": false,
          "title": ""
        },
        "id": "dlqXXN8oVaTG"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
        "                                         output_directory_prompt,\n",
        "                                         #device_map=device,\n",
        "                                         is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjXT-6EKMiXk"
      },
      "outputs": [],
      "source": [
        "loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n",
        "                                          input_prompt,\n",
        "                                          max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzjDgDE2reTO"
      },
      "source": [
        "Let's compare the result of the model before and after being fine-tuned with prompt-tuning.\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:  Follow up with your trainer\n",
        "```\n",
        "**560M Trained for 50 Epochs & 8 virtual tokens:**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt: ＋ Acts like an expert in the field of sports and health, but does not provide detailed information about his work or products to help you understand them better.  + I want my first client referred me through this website for their gym membership program which is based on physical activity training exercises that are easy enough (eight minutes) per week with no need any special equipment required.   - First Question : What would be your role?\n",
        "```\n",
        "\n",
        "**560M Trained for  50 Epochs & 20 virtual tokens:**\n",
        "```\n",
        "'Act as a fitness Trainer. Prompt:  I want you to act like an expert in your field and provide me with advice on how best improving my health, wellness or even getting fit for the upcoming season of sports activities such as: - Running (running); Pilates exercise'\n",
        "```\n",
        "\n",
        "It's very clear that the result is quite different, it's not exactly what we're looking for but it's much closer.\n",
        "\n",
        "It's possible that we're at the limit of what Bloom's smallest model can offer. Try with any other model, surely with the one with 1B parameters the result will be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVif-42UCP7l"
      },
      "source": [
        "# Hate Classifier\n",
        "##Loading the Dataset\n",
        "\n",
        "* https://huggingface.co/datasets/SetFit/ethos_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyp64F0tRQBt"
      },
      "outputs": [],
      "source": [
        "input_classifier = tokenizer(\"Sentence : Head is the shape of a light bulb. Label : \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_classifier.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gk7C_NSt-Y"
      },
      "source": [
        "The model has no idea what its purpose is, so it completes the sentence as best as it can."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlLAsUvhE09L"
      },
      "outputs": [],
      "source": [
        "dataset_classifier = \"SetFit/ethos_binary\"\n",
        "\n",
        "def concatenate_columns_classifier(dataset):\n",
        "    def concatenate(example):\n",
        "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y2K_3MqE4QV"
      },
      "outputs": [],
      "source": [
        "data_classifier = load_dataset(dataset_classifier)\n",
        "data_classifier['train'] = concatenate_columns_classifier(\n",
        "    data_classifier['train'])\n",
        "\n",
        "data_classifier = data_classifier.map(\n",
        "    lambda samples: tokenizer(samples[\"text\"]),\n",
        "    batched=True)\n",
        "train_sample_classifier = data_classifier[\"train\"].remove_columns(\n",
        "    ['label', 'label_text', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia-vz3ddTOY5"
      },
      "outputs": [],
      "source": [
        "data_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ROxvTEFAVC"
      },
      "outputs": [],
      "source": [
        "train_sample_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2Z_LiRTMDC"
      },
      "source": [
        "I have deleted all the columns from the dataset that are not strictly necessary for training, that is to say, I have removed all columns that are not essential for the model's learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFUZNrAdFDR5"
      },
      "outputs": [],
      "source": [
        "print(train_sample_classifier[2:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_F3oR70FF4z"
      },
      "source": [
        "## prompt-tuning configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr0Aw_byFL6n"
      },
      "outputs": [],
      "source": [
        "generation_config_classifier = PromptTuningConfig(\n",
        "    #This type indicates the model will generate text.\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    prompt_tuning_init_text=\"Indicate if the text contains hate speech or no hate speech.\",\n",
        "    #Number of virtual tokens to be added and trained.\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
        "    #The pre-trained model.\n",
        "    tokenizer_name_or_path=model_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWvoJMajFPcg"
      },
      "outputs": [],
      "source": [
        "peft_model_classifier = get_peft_model(\n",
        "    foundational_model,\n",
        "    generation_config_classifier)\n",
        "print(peft_model_classifier.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpKtEudsFWTq"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_directory_classifier):\n",
        "    os.mkdir(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DknsYEGwFW4g"
      },
      "outputs": [],
      "source": [
        "training_args_classifier = create_training_arguments(\n",
        "    output_directory_classifier,\n",
        "    3e-2,\n",
        "    NUM_EPOCHS_CLASSIFIER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgEAjAMFasw"
      },
      "source": [
        "## Training Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfDLVy22FaNs"
      },
      "outputs": [],
      "source": [
        "trainer_classifier = create_trainer(peft_model_classifier,\n",
        "                                   training_args_classifier,\n",
        "                                   train_sample_classifier)\n",
        "trainer_classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcWL4-4OKCx9"
      },
      "outputs": [],
      "source": [
        "trainer_classifier.model.save_pretrained(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRcSaXMmM3mz"
      },
      "source": [
        "## Inference second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pkx0npHNWja"
      },
      "outputs": [],
      "source": [
        "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "loaded_model_peft.set_adapter(\"classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an8T4LvPJAUC"
      },
      "outputs": [],
      "source": [
        "loaded_model_sentences_outputs = get_outputs(loaded_model_peft,\n",
        "                                             input_classifier,\n",
        "                                             max_new_tokens=3)\n",
        "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbeFTXjVaTG"
      },
      "source": [
        "Let's check how the model's response has changed with training:\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :\n",
        "Sentence : I don't like short people, no idea why they exist. Label :\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  head\n",
        "Sentence : I don't like short people, no idea why they exist. Label :  No\n",
        "```\n",
        "**Trained for classification with Prompt-tuning**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  no hate speech\n",
        "Sentence : I don't like short people, no idea why they exist. Label :  hate speech\n",
        "```\n",
        "\n",
        "It's clear that the training has fulfilled its purpose. The original model doesn't know what its mission is and tries to complete the sentences as best as it can. On the other hand, the updated model with prompt-tuning does know what its mission is and is able to classify the sentences correctly and in the indicated format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TUjNtGVaTH"
      },
      "source": [
        "# Conclusion\n",
        "Prompt Tuning is an amazing technique that can save us hours of training and a significant amount of money. In the notebook, we have trained two models in just a few minutes, and we can have both models in memory, providing service to different clients.\n",
        "\n",
        "If you want to try different combinations and models, the notebook is ready to use another model from the Bloom family.\n",
        "\n",
        "You can change the number of epochs to train, the number of virtual tokens, and the model. However, there are many configurations to change.\n",
        "\n",
        "*The responses of the fine-tuned models may vary every time we train them. I've pasted the results of one of my trainings, but the actual results may differ.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OMyCWasVaTH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}