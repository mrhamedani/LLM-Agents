{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/LLM-Agents/blob/main/13_Tracing_%26_Evaluating_with_LangSmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2f8fcc-0d7f-422b-8d23-6e2e70a7c4a5",
      "metadata": {
        "id": "6c2f8fcc-0d7f-422b-8d23-6e2e70a7c4a5"
      },
      "source": [
        "\n",
        "Models: t5-base-cnn / t5-base / OpenAI\n",
        "\n",
        "Colab Environment: CPU\n",
        "\n",
        "Keys:\n",
        "* Embeddings\n",
        "* LangSmith\n",
        "\n",
        "\n",
        " If you want to run this notebook, you will need to have API keys from OpenAI, HuggingFace, and LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a5ae21-8eaf-431c-bb2f-22a8dc8b7e47",
      "metadata": {
        "id": "a1a5ae21-8eaf-431c-bb2f-22a8dc8b7e47"
      },
      "outputs": [],
      "source": [
        "#Loading Necessary Libraries\n",
        "!pip install -q transformers==4.42.4\n",
        "!pip install -q langchain==0.2.11\n",
        "!pip install -q langchain-openai==0.1.19\n",
        "!pip install -q langchainhub==0.1.20\n",
        "!pip install -q datasets==2.20.0\n",
        "!pip install -q huggingface-hub==0.23.5\n",
        "!pip install -q langchain-community==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db4887a-a214-49bc-9bb6-0ae59c1092ec",
      "metadata": {
        "id": "6db4887a-a214-49bc-9bb6-0ae59c1092ec"
      },
      "outputs": [],
      "source": [
        "#You need a LangChain API Key.\n",
        "from getpass import getpass\n",
        "import os\n",
        "if not 'LANGCHAIN_API_KEY' in os.environ:\n",
        "  os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"LangChain API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8e338c-45e2-42d1-94f2-64b70cc4b529",
      "metadata": {
        "id": "ca8e338c-45e2-42d1-94f2-64b70cc4b529"
      },
      "outputs": [],
      "source": [
        "if not 'OPENAI_API_KEY' in os.environ:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4adff9-1169-4173-a72b-c983dcb6160d",
      "metadata": {
        "id": "5f4adff9-1169-4173-a72b-c983dcb6160d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "#os.environ[\"LANGCHAIN_PROJECT\"]=\"langsmith_yttest01\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0189af0e-8145-407b-a32d-2f21ac0ca5ff",
      "metadata": {
        "id": "0189af0e-8145-407b-a32d-2f21ac0ca5ff"
      },
      "outputs": [],
      "source": [
        "#Importing Client from Langsmith\n",
        "from langsmith import Client\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7c4be2-fdae-42cf-b1b2-9aef89081ad2",
      "metadata": {
        "id": "4e7c4be2-fdae-42cf-b1b2-9aef89081ad2"
      },
      "source": [
        "## Create Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc219ae-1760-4f3d-92e6-aaeb6fb7ee9f",
      "metadata": {
        "id": "dbc219ae-1760-4f3d-92e6-aaeb6fb7ee9f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "cnn_dataset = load_dataset(\n",
        "    \"ccdv/cnn_dailymail\", version\n",
        "    =\"3.0.0\",\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7684f9ef-f665-42c1-ac89-f2726f7ae0da",
      "metadata": {
        "id": "7684f9ef-f665-42c1-ac89-f2726f7ae0da"
      },
      "outputs": [],
      "source": [
        "def add_prefix(example):\n",
        "    return {\n",
        "        **example,\n",
        "        \"article\": f\"Summarize this news:\\n{example['article']}\"\n",
        "    }\n",
        "\n",
        "#cnn_dataset = cnn_dataset.map(add_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc26871-68d9-4ab1-a9c9-b6caf241d636",
      "metadata": {
        "id": "3fc26871-68d9-4ab1-a9c9-b6caf241d636"
      },
      "outputs": [],
      "source": [
        "#Get just a few news to test\n",
        "MAX_NEWS=3\n",
        "sample_cnn = cnn_dataset[\"test\"].select(range(MAX_NEWS)).map(add_prefix)\n",
        "\n",
        "sample_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0ef576-ea25-446d-a99b-354678a274ba",
      "metadata": {
        "id": "bc0ef576-ea25-446d-a99b-354678a274ba"
      },
      "source": [
        "The dataset contains three columns: article, highlights, and id. To use LangSmith, we need to create a dataset in LangSmith format.\n",
        "\n",
        "LangSmith expects a prompt and a result. To achieve this, we will transform the article into a prompt by adding the prefix: \"Summarize this news.\" As a result, we will use the content of highlights, which represents the summaries created by humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3cea3a-fde5-4e52-90f9-d1db4fd30bc2",
      "metadata": {
        "id": "4e3cea3a-fde5-4e52-90f9-d1db4fd30bc2"
      },
      "outputs": [],
      "source": [
        "print(sample_cnn[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1051e780-8871-4fd4-9e18-68ad8bc837f8",
      "metadata": {
        "id": "1051e780-8871-4fd4-9e18-68ad8bc837f8"
      },
      "source": [
        "Now We have the Dataset with the prompt and the Reference Summary, it is time to create a Dataset in LangSmith with this information.\n",
        "### Create the Dataset in Langsmith\n",
        "\n",
        "The dataset in LangSmith is composed of an input, which is the prompt passed to the model for evaluation, and an output, which should contain what we expect the model to return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221d8e92-c2f1-499c-9e68-3abb752722c4",
      "metadata": {
        "id": "221d8e92-c2f1-499c-9e68-3abb752722c4"
      },
      "outputs": [],
      "source": [
        "#import uuid\n",
        "import datetime\n",
        "input_key=['article']\n",
        "output_key=['highlights']\n",
        "\n",
        "NAME_DATASET=f\"Summarize_dataset_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf23c41-e15d-4f63-bd27-233d6599e305",
      "metadata": {
        "id": "dcf23c41-e15d-4f63-bd27-233d6599e305"
      },
      "outputs": [],
      "source": [
        "#This create the Dataset in LangSmith with the content in sample_cnn\n",
        "dataset = client.upload_dataframe(\n",
        "    df=sample_cnn,\n",
        "    input_keys=input_key,\n",
        "    output_keys=output_key,\n",
        "    name=NAME_DATASET,\n",
        "    description=\"Test Embedding distance between model summarizations\",\n",
        "    data_type=\"kv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada2bd55-b6bc-467c-b481-48d46ad50441",
      "metadata": {
        "id": "ada2bd55-b6bc-467c-b481-48d46ad50441"
      },
      "source": [
        "In this image, we can see an example from the dataset once it's been registered in LangSmith.\n",
        "\n",
        "In the Input column, there is the prompt to be sent, while in the Output column, the expected output is stored.\n",
        "\n",
        "When performing the comparison, the model will be given the prompt, and the Cosine distance between its response and the one stored in the sample dataset will be calculated.\n",
        "<img src=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_4_2SDL_Dataset.jpg?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8278f53f-1a94-407c-846d-05c926737704",
      "metadata": {
        "id": "8278f53f-1a94-407c-846d-05c926737704"
      },
      "source": [
        "### Recovering Models From Hugging Face\n",
        "Let's retrieve both models from HuggingFace. A base T5 model and a model that has been fine-tuned using the training portion of this same dataset to generate summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1afd20-0bbe-473b-9ad9-ebceb3519421",
      "metadata": {
        "id": "0f1afd20-0bbe-473b-9ad9-ebceb3519421"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5213c43f-85e2-4159-8a8e-684e79850bae",
      "metadata": {
        "id": "5213c43f-85e2-4159-8a8e-684e79850bae"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "hf_key = getpass(\"Hugging Face Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12339bcc-ce70-4fae-9dcb-f2f273429ed8",
      "metadata": {
        "id": "12339bcc-ce70-4fae-9dcb-f2f273429ed8"
      },
      "outputs": [],
      "source": [
        "summarizer_base = HuggingFaceHub(\n",
        "    repo_id=\"t5-base\",\n",
        "    model_kwargs={\"temperature\":0, \"max_length\":180},\n",
        "    huggingfacehub_api_token=hf_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f317d1cc-95f0-4f71-9c7f-205e42afc5df",
      "metadata": {
        "id": "f317d1cc-95f0-4f71-9c7f-205e42afc5df"
      },
      "outputs": [],
      "source": [
        "summarizer_finetuned = HuggingFaceHub(\n",
        "    repo_id=\"flax-community/t5-base-cnn-dm\",\n",
        "    model_kwargs={\"temperature\":0, \"max_length\":180},\n",
        "    huggingfacehub_api_token=hf_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc859325-3e92-4f2b-85cc-247dd84d9cf6",
      "metadata": {
        "id": "dc859325-3e92-4f2b-85cc-247dd84d9cf6"
      },
      "source": [
        "## Defining Evaluator\n",
        "The first step is to define an evaluator, where we specify the variables we want to evaluate. In our case, I have chosen to measure only the \"embedding_distance.\"\n",
        "\n",
        "I've left the \"string_distance\" as a comment in case you want to conduct a test with two evaluations instead of one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d506d770-2c30-41a6-acde-283ad1b27ea9",
      "metadata": {
        "id": "d506d770-2c30-41a6-acde-283ad1b27ea9"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import run_on_dataset, RunEvalConfig\n",
        "!pip install -q rapidfuzz==3.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e8accd-d7e1-44f8-b5d0-0215ceedb88d",
      "metadata": {
        "id": "15e8accd-d7e1-44f8-b5d0-0215ceedb88d"
      },
      "outputs": [],
      "source": [
        "#We are using just one of the multiple evaluator available on LangSmith.\n",
        "evaluation_config = RunEvalConfig(\n",
        "    evaluators=[\n",
        "        \"embedding_distance\",\n",
        "        #RunEvalConfig.Criteria(\"conciseness\"),\n",
        "        #RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        #\"string_distance\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee666c30-d958-45b1-802c-7034dd13b42b",
      "metadata": {
        "id": "ee666c30-d958-45b1-802c-7034dd13b42b"
      },
      "source": [
        "## Running Evaluator\n",
        "With the same configuration, we can launch two evaluations on the same dataset. One for each of the chosen models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EnrXICCZSVy1",
      "metadata": {
        "id": "EnrXICCZSVy1"
      },
      "outputs": [],
      "source": [
        "chain_models = [summarizer_base, summarizer_finetuned]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c75c07-8351-465f-af15-a1abe17968f9",
      "metadata": {
        "id": "67c75c07-8351-465f-af15-a1abe17968f9"
      },
      "outputs": [],
      "source": [
        "project_name = f\"T5-BASE {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "base_t5_results = run_on_dataset(\n",
        "    client=client,\n",
        "    project_name=project_name,\n",
        "    dataset_name=NAME_DATASET,\n",
        "    llm_or_chain_factory=summarizer_base,\n",
        "    evaluation=evaluation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ede4d6-ed51-4745-b91a-d9ec6c40d436",
      "metadata": {
        "id": "76ede4d6-ed51-4745-b91a-d9ec6c40d436"
      },
      "outputs": [],
      "source": [
        "project_name = f\"T5-FineTuned {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "finetuned_t5_results = run_on_dataset(\n",
        "    client=client,\n",
        "    project_name=project_name,\n",
        "    dataset_name=NAME_DATASET,\n",
        "    llm_or_chain_factory=summarizer_finetuned,\n",
        "    evaluation=evaluation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953211fe-b8a4-4a2c-91e3-3c6242a2f159",
      "metadata": {
        "id": "953211fe-b8a4-4a2c-91e3-3c6242a2f159"
      },
      "source": [
        "<img src=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_4_2SDL_Tests.jpg?raw=true\">\n",
        "\n",
        "In the image below you can see the comparision between two tests.\n",
        "<img src=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_4_2SDL_CompareTestst.jpg?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xn6jeX7u_4MR",
      "metadata": {
        "id": "Xn6jeX7u_4MR"
      },
      "source": [
        "Well, since it has been so straightforward, why don't we try to make the comparison with an OpenAI model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z4sYgAASO7_k",
      "metadata": {
        "id": "z4sYgAASO7_k"
      },
      "outputs": [],
      "source": [
        "if not 'OPENAI_API_KEY' in os.environ:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84a97ac1-623e-4d28-a78b-c807af89c918",
      "metadata": {
        "id": "84a97ac1-623e-4d28-a78b-c807af89c918"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "open_aillm=OpenAI(temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suc0G-9W94Qj",
      "metadata": {
        "id": "suc0G-9W94Qj"
      },
      "outputs": [],
      "source": [
        "project_name = f\"OpenAI {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "finetuned_t5_results = run_on_dataset(\n",
        "    client=client,\n",
        "    project_name=project_name,\n",
        "    dataset_name=NAME_DATASET,\n",
        "    llm_or_chain_factory=open_aillm,\n",
        "    evaluation=evaluation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B15V_-3pBvOK",
      "metadata": {
        "id": "B15V_-3pBvOK"
      },
      "source": [
        "<img src=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_4_2SDL_CompareOpenAI_HF.jpg?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fUeZWvLMCOBw",
      "metadata": {
        "id": "fUeZWvLMCOBw"
      },
      "source": [
        "The experiment with the OpenAI model has yielded the best results. But, be aware! As we can see, there is a cost involved since we are using an API, and it needs to be paid for.\n",
        "\n",
        "Another crucial piece of information is that we can view performance data for the models. This data could also be useful for minimally evaluating our inference server."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}